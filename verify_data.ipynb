{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Data Verification\n",
        "\n",
        "This notebook verifies the generated parquet files and their contents to ensure everything is as expected.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checking file existence:\n",
            "✓ train_data.parquet\n",
            "✓ val_data.parquet\n",
            "✓ test_data.parquet\n",
            "✓ metadata.parquet\n",
            "✓ title_embeddings.parquet\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import yaml\n",
        "\n",
        "# Load config\n",
        "with open('config.yaml', 'r') as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "# Define paths\n",
        "data_path = Path(config['output']['base_path'])\n",
        "train_file = data_path / config['output']['train_file']\n",
        "val_file = data_path / config['output']['val_file']\n",
        "test_file = data_path / config['output']['test_file']\n",
        "metadata_file = data_path / config['output']['metadata_file']\n",
        "embeddings_file = data_path / config['output']['embeddings_file']\n",
        "\n",
        "print(\"Checking file existence:\")\n",
        "for file in [train_file, val_file, test_file, metadata_file, embeddings_file]:\n",
        "    print(f\"✓ {file.name}\" if file.exists() else f\"✗ {file.name} (missing)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Verifying data files:\n",
            "\n",
            "Train data:\n",
            "- Records: 2029\n",
            "- Unique users: 253\n",
            "- Unique items: 341\n",
            "\n",
            "Columns: ['user_id', 'parent_asin', 'rating', 'timestamp', 'history']\n",
            "\n",
            "Sample data:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>parent_asin</th>\n",
              "      <th>rating</th>\n",
              "      <th>timestamp</th>\n",
              "      <th>history</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AFSKPY37N3C43SOI5IEXEK5JSIYA</td>\n",
              "      <td>B07J3GH1W1</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1547589356557</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>AFSKPY37N3C43SOI5IEXEK5JSIYA</td>\n",
              "      <td>B07W397QG4</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1593352422858</td>\n",
              "      <td>B07J3GH1W1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                        user_id parent_asin rating      timestamp     history\n",
              "0  AFSKPY37N3C43SOI5IEXEK5JSIYA  B07J3GH1W1    5.0  1547589356557            \n",
              "1  AFSKPY37N3C43SOI5IEXEK5JSIYA  B07W397QG4    5.0  1593352422858  B07J3GH1W1"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation data:\n",
            "- Records: 253\n",
            "- Unique users: 253\n",
            "- Unique items: 168\n",
            "\n",
            "Test data:\n",
            "- Records: 253\n",
            "- Unique users: 253\n",
            "- Unique items: 138\n",
            "\n",
            "Verifying user overlap between splits:\n",
            "- Train-Val overlap: 253 users\n",
            "- Train-Test overlap: 253 users\n",
            "- Val-Test overlap: 253 users\n"
          ]
        }
      ],
      "source": [
        "# Load and verify data files\n",
        "print(\"\\nVerifying data files:\")\n",
        "\n",
        "# Load train data\n",
        "train_df = pd.read_parquet(train_file)\n",
        "print(\"\\nTrain data:\")\n",
        "print(f\"- Records: {len(train_df)}\")\n",
        "print(f\"- Unique users: {train_df['user_id'].nunique()}\")\n",
        "print(f\"- Unique items: {train_df['parent_asin'].nunique()}\")\n",
        "print(\"\\nColumns:\", train_df.columns.tolist())\n",
        "print(\"\\nSample data:\")\n",
        "display(train_df.head(2))\n",
        "\n",
        "# Load validation data\n",
        "val_df = pd.read_parquet(val_file)\n",
        "print(\"\\nValidation data:\")\n",
        "print(f\"- Records: {len(val_df)}\")\n",
        "print(f\"- Unique users: {val_df['user_id'].nunique()}\")\n",
        "print(f\"- Unique items: {val_df['parent_asin'].nunique()}\")\n",
        "\n",
        "# Load test data\n",
        "test_df = pd.read_parquet(test_file)\n",
        "print(\"\\nTest data:\")\n",
        "print(f\"- Records: {len(test_df)}\")\n",
        "print(f\"- Unique users: {test_df['user_id'].nunique()}\")\n",
        "print(f\"- Unique items: {test_df['parent_asin'].nunique()}\")\n",
        "\n",
        "# Verify no overlap between splits\n",
        "train_users = set(train_df['user_id'])\n",
        "val_users = set(val_df['user_id'])\n",
        "test_users = set(test_df['user_id'])\n",
        "\n",
        "print(\"\\nVerifying user overlap between splits:\")\n",
        "print(f\"- Train-Val overlap: {len(train_users & val_users)} users\")\n",
        "print(f\"- Train-Test overlap: {len(train_users & test_users)} users\")\n",
        "print(f\"- Val-Test overlap: {len(val_users & test_users)} users\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify metadata and embeddings\n",
        "print(\"\\nVerifying metadata and embeddings:\")\n",
        "\n",
        "# Load metadata\n",
        "metadata_df = pd.read_parquet(metadata_file)\n",
        "print(\"\\nMetadata:\")\n",
        "print(f\"- Records: {len(metadata_df)}\")\n",
        "print(f\"- Unique parent ASINs: {metadata_df['parent_asin'].nunique()}\")\n",
        "print(\"\\nColumns:\", metadata_df.columns.tolist())\n",
        "print(\"\\nSample data:\")\n",
        "display(metadata_df.head(2))\n",
        "\n",
        "# Load embeddings\n",
        "embeddings_df = pd.read_parquet(embeddings_file)\n",
        "print(\"\\nEmbeddings:\")\n",
        "print(f\"- Records: {len(embeddings_df)}\")\n",
        "print(f\"- Unique parent ASINs: {embeddings_df['parent_asin'].nunique()}\")\n",
        "print(f\"- Embedding dimension: {len(embeddings_df['embedding'].iloc[0])}\")\n",
        "print(\"\\nColumns:\", embeddings_df.columns.tolist())\n",
        "\n",
        "# Verify coverage\n",
        "all_parent_asins = set(pd.concat([train_df, val_df, test_df])['parent_asin'].unique())\n",
        "metadata_asins = set(metadata_df['parent_asin'])\n",
        "embeddings_asins = set(embeddings_df['parent_asin'])\n",
        "\n",
        "print(\"\\nVerifying coverage:\")\n",
        "print(f\"- Total unique parent ASINs in data: {len(all_parent_asins)}\")\n",
        "print(f\"- Parent ASINs with metadata: {len(metadata_asins)} ({len(metadata_asins & all_parent_asins)} in data)\")\n",
        "print(f\"- Parent ASINs with embeddings: {len(embeddings_asins)} ({len(embeddings_asins & all_parent_asins)} in data)\")\n",
        "\n",
        "# Check for missing titles or embeddings\n",
        "missing_metadata = all_parent_asins - metadata_asins\n",
        "missing_embeddings = all_parent_asins - embeddings_asins\n",
        "\n",
        "if missing_metadata:\n",
        "    print(\"\\nWarning: Missing metadata for ASINs:\", list(missing_metadata)[:5], \"...\")\n",
        "if missing_embeddings:\n",
        "    print(\"\\nWarning: Missing embeddings for ASINs:\", list(missing_embeddings)[:5], \"...\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
